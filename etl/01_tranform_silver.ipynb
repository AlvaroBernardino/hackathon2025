{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cf176-5f71-478c-8ffc-d99e0490da87",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Bibliotecas padrão\n",
    "import os\n",
    "from io import BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Bibliotecas externas\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, row_number, expr, monotonically_increasing_id, \n",
    "    regexp_replace, trim, lower, to_date, date_format, \n",
    "    dayofmonth, month, year, quarter, lag, when, sequence, \n",
    "    explode, count, sum as spark_sum\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a76b30-f1ce-488d-a1b8-5cb7c73b7a32",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Dicionários e variáveis\n",
    "path_silver = \"abfss://Hackathon_Emprega_Dados@onelake.dfs.fabric.microsoft.com/lakehouse_vendas.Lakehouse/Files/silver/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae791049-fe58-443d-855c-ccad6bb86bb6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "## Função para converter a coluna de data para o formato id_data\n",
    "def convert_iddata(df, col_name):\n",
    "    return pd.to_datetime(df[col_name], format=\"%Y%m%d\").dt.strftime(\"%Y%m%d\").astype(\"Int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b20ec4-a6de-4d30-84cd-92cc7da491cf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def classificar_despesa(item):\n",
    "    if item in [\n",
    "        'Despesas com matéria-prima',\n",
    "        'Despesas de transporte e logística',\n",
    "        'Mecanicos',\n",
    "        'Manutenção de equipamentos'\n",
    "    ]:\n",
    "        return 'Custos Variáveis (CPV)'\n",
    "\n",
    "    elif item in [\n",
    "        'Despesas com marketing e publicidade',\n",
    "        'Despesas com viagens e deslocamentos',\n",
    "        'Despesas com telefonia móvel e fixa',\n",
    "        'Trafego', 'Google Ads', 'Meta Ads', 'LinkedIn Ads', 'CRM', 'Landin page'\n",
    "    ]:\n",
    "        return 'Despesas Comerciais'\n",
    "\n",
    "    elif item in [\n",
    "        'Impostos e taxas'\n",
    "    ]:\n",
    "        return 'Impostos'\n",
    "\n",
    "    elif item in [\n",
    "        'Treinamento'\n",
    "    ]:\n",
    "        return 'Desenvolvimento Pessoal'\n",
    "\n",
    "    else:\n",
    "        return 'Despesas Administrativas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624344fc-67a3-480d-8c71-5d9c591a5591",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializa a SparkSession (se ainda não o fez)\n",
    "spark = SparkSession.builder.appName(\"ClientesUnicos\").getOrCreate()\n",
    "\n",
    "# 1. Leitura das tabelas Delta da camada bronze\n",
    "df_clientes_consig = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"Tables/b_vendas_clientes_consigcar\")\n",
    "    .selectExpr(\"Nome as nome_cliente\", \"WhatsApp as whatsapp\")\n",
    "    .distinct()\n",
    ")\n",
    "df_clientes_consig = df_clientes_consig.withColumn(\"empresa\", lit(\"Consigcar\"))\n",
    "\n",
    "df_clientes_alu = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"Tables/b_vendas_clientes_alucar\")\n",
    "    .selectExpr(\"`Nome__Alucar_` as nome_cliente\")\n",
    "    .distinct()\n",
    ")\n",
    "df_clientes_alu = (\n",
    "    df_clientes_alu.withColumn(\"whatsapp\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"empresa\", lit(\"Alucar\"))\n",
    ")\n",
    "\n",
    "# 2. Combina as duas tabelas\n",
    "df_clientes = df_clientes_consig.unionByName(df_clientes_alu)\n",
    "\n",
    "\n",
    "# 3. Remover valores duplicados e adicionar 'id_cliente' na primeira posição\n",
    "\n",
    "# Remove duplicatas primeiro\n",
    "df_clientes_unicos = df_clientes.dropDuplicates([\"nome_cliente\"])\n",
    "\n",
    "# Adiciona a coluna 'id_cliente'\n",
    "window_spec = Window.orderBy(\"nome_cliente\")\n",
    "df_clientes_com_id = df_clientes_unicos.withColumn(\"id_cliente\", row_number().over(window_spec) - 1)\n",
    "\n",
    "# Pega todas as colunas existentes, exceto 'id_cliente' (se por algum motivo já existisse antes)\n",
    "# Isso garante que não teremos duas colunas 'id_cliente' ao fazer o select final.\n",
    "colunas_existentes = [col for col in df_clientes_com_id.columns if col != \"id_cliente\"]\n",
    "\n",
    "# Cria a lista final de colunas com 'id_cliente' na primeira posição\n",
    "colunas_finais = [\"id_cliente\"] + colunas_existentes\n",
    "\n",
    "# Seleciona as colunas na ordem desejada\n",
    "df_clientes_final = df_clientes_com_id.select(*colunas_finais).sort(\"id_cliente\")\n",
    "\n",
    "# 4. Salva a tabela na camada silver como Delta\n",
    "df_clientes_final.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_dim_cliente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42b32d-2975-463f-9f5e-b2a49ca2c783",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Leitura da tabela Delta na camada bronze\n",
    "df_vend = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"Tables/b_vendas_clientes_consigcar\")\n",
    "    .select(\"Vendedor\")\n",
    "    .distinct()\n",
    "    .withColumnRenamed(\"Vendedor\", \"nome_vendedor\")\n",
    ")\n",
    "\n",
    "# Adiciona id_cliente único\n",
    "df_vend = df_vend.withColumn(\"id_vendedor\", monotonically_increasing_id())\n",
    "\n",
    "colunas_existentes = [col for col in df_vend.columns if col != \"id_vendedor\"]\n",
    "\n",
    "# Cria a lista final de colunas com 'id_cliente' na primeira posição\n",
    "colunas_finais = [\"id_vendedor\"] + colunas_existentes\n",
    "\n",
    "# Seleciona as colunas na ordem desejada\n",
    "df_vendedor_final = df_vend.select(*colunas_finais).sort(\"id_vendedor\")\n",
    "\n",
    "# Salvar na camada silver como tabela Delta (substitui se já existir)\n",
    "df_vendedor_final.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_dim_vendedor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60786cfe-a2be-4324-9e48-3215ca65e1ab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Criar sessão Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Gerar intervalo de datas de 2018-01-01 a 2030-12-31\n",
    "start_date = datetime(2018, 1, 1)\n",
    "end_date = datetime(2030, 12, 31)\n",
    "\n",
    "date_list = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_list.append((int(current_date.strftime('%Y%m%d')), current_date))\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Criar DataFrame Spark\n",
    "df_tempo = spark.createDataFrame(date_list, [\"id_data\", \"data\"])\n",
    "\n",
    "# Adicionar colunas\n",
    "df_tempo = df_tempo.withColumn(\"dia\", dayofmonth(\"data\")) \\\n",
    "                   .withColumn(\"mes\", month(\"data\")) \\\n",
    "                   .withColumn(\"ano\", year(\"data\")) \\\n",
    "                   .withColumn(\"nome_mes\", date_format(\"data\", \"MMMM\")) \\\n",
    "                   .withColumn(\"dia_da_semana\", date_format(\"data\", \"EEEE\")) \\\n",
    "                   .withColumn(\"trimestre\", quarter(\"data\"))\n",
    "\n",
    "# (Opcional) Forçar locale pt_BR se Spark estiver configurado\n",
    "# Isso depende do locale configurado no ambiente do cluster.\n",
    "# Ex: spark.conf.set(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")\n",
    "\n",
    "# Salvar como tabela Delta (Silver Layer)\n",
    "df_tempo.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"s_dim_tempo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f830c12-0efd-4b6b-9837-d19ecefc0535",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) Ler as duas tabelas Delta\n",
    "df_alucar = spark.read.format(\"delta\").load(\"Tables/b_despesas_alucar\") \\\n",
    "    .select(col(\"DESPESAS\").alias(\"nome_despesa\"), col(\"Valor\").alias(\"valor\"), col(\"Mês\").alias(\"mes\")) \\\n",
    "    .withColumn(\"origem\", lit(\"Alucar\"))\n",
    "\n",
    "df_consig = spark.read.format(\"delta\").load(\"Tables/b_despesas_consigcar\") \\\n",
    "    .select(col(\"DESPESAS\").alias(\"nome_despesa\"), col(\"Valor\").alias(\"valor\"), col(\"Mês\").alias(\"mes\")) \\\n",
    "    .withColumn(\"origem\", lit(\"Consigcar\"))\n",
    "\n",
    "# 2) Concatenar\n",
    "df_desp = df_alucar.unionByName(df_consig)\n",
    "\n",
    "# 3) Limpar coluna valor: remove 'R$', pontos, troca vírgula por ponto, trim, converte para float\n",
    "df_desp = df_desp.withColumn(\n",
    "    \"valor\",\n",
    "    regexp_replace(regexp_replace(regexp_replace(trim(col(\"valor\")), \"R\\\\$\", \"\"), \"\\\\.\", \"\"), \",\", \".\").cast(FloatType())\n",
    ")\n",
    "\n",
    "# 4) Criar função para mapear mês em português para número\n",
    "meses = {\n",
    "    'janeiro': '01', 'fevereiro': '02', 'março': '03', 'abril': '04',\n",
    "    'maio': '05', 'junho': '06', 'julho': '07', 'agosto': '08',\n",
    "    'setembro': '09', 'outubro': '10', 'novembro': '11', 'dezembro': '12'\n",
    "}\n",
    "\n",
    "def converte_mes_udf(mes):\n",
    "    if mes is None:\n",
    "        return None\n",
    "    mes = mes.lower().strip()\n",
    "    for nome, numero in meses.items():\n",
    "        if mes.startswith(nome):\n",
    "            return f\"2025-{numero}-01\"\n",
    "    return None\n",
    "\n",
    "udf_converte_mes = udf(converte_mes_udf, StringType())\n",
    "\n",
    "df_desp = df_desp.withColumn(\"data\", udf_converte_mes(col(\"mes\")))\n",
    "\n",
    "# 5) Converter para data e depois gerar id_data YYYYMMDD (inteiro)\n",
    "df_desp = df_desp.withColumn(\"data_dt\", to_date(col(\"data\"), \"yyyy-MM-dd\"))\n",
    "df_desp = df_desp.withColumn(\"id_data\", date_format(col(\"data_dt\"), \"yyyyMMdd\").cast(IntegerType()))\n",
    "\n",
    "# 6) Criar coluna categoria - você precisa definir a função classificar_despesa para PySpark\n",
    "# Se for simples, pode fazer um UDF, exemplo abaixo com placeholder\n",
    "\n",
    "udf_classificar = udf(classificar_despesa, StringType())\n",
    "\n",
    "df_desp = df_desp.withColumn(\"categoria\", udf_classificar(col(\"nome_despesa\")))\n",
    "\n",
    "window_spec = Window.orderBy(\"nome_despesa\")\n",
    "df_despesa_com_id = df_desp.withColumn(\"id_despesa\", row_number().over(window_spec) - 1)\n",
    "\n",
    "colunas_existentes = [col for col in df_despesa_com_id.columns if col != \"id_despesa\"]\n",
    "\n",
    "# Cria a lista final de colunas com 'id_cliente' na primeira posição\n",
    "colunas_finais = [\"id_despesa\"] + colunas_existentes\n",
    "\n",
    "# Seleciona as colunas na ordem desejada\n",
    "df_despesa_final = df_despesa_com_id.select(*colunas_finais).sort(\"id_despesa\")\n",
    "\n",
    "# 7) Selecionar colunas para salvar\n",
    "df_final = df_despesa_final.select(\"id_despesa\", \"origem\", \"categoria\", \"nome_despesa\", \"valor\", \"id_data\")\n",
    "\n",
    "# 8) Salvar na camada silver como Delta\n",
    "df_final.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_despesas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a108b-fd65-44ad-b972-84ba1d76157a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Ler a tabela Delta\n",
    "df_pag = spark.read.format(\"delta\").load(\"Tables/b_receita_pagseguro_consigcar\") \\\n",
    "    .select(col(\"Data\").alias(\"data\"), col(\"Valor\").alias(\"valor_faturado\"))\n",
    "\n",
    "# 2) Converter data para id_data no formato YYYYMMDD inteiro\n",
    "df_pag = df_pag.withColumn(\"data_dt\", to_date(col(\"data\"), \"yyyy-MM-dd\"))  # Ajuste o formato se necessário\n",
    "df_pag = df_pag.withColumn(\"id_data\", date_format(col(\"data_dt\"), \"yyyyMMdd\").cast(IntegerType()))\n",
    "\n",
    "# 3) Limpar e converter valor_faturado para float\n",
    "df_pag = df_pag.withColumn(\n",
    "    \"valor_faturado\",\n",
    "    regexp_replace(regexp_replace(regexp_replace(trim(col(\"valor_faturado\")), \"R\\\\$\", \"\"), \"\\\\.\", \"\"), \",\", \".\").cast(FloatType())\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(\"id_data\")\n",
    "df_pag = df_pag.withColumn(\"id_faturamento\", row_number().over(window_spec) - 1)\n",
    "\n",
    "colunas_existentes = [col for col in df_pag.columns if col != \"id_faturamento\"]\n",
    "\n",
    "# Cria a lista final de colunas com 'id_cliente' na primeira posição\n",
    "colunas_finais = [\"id_faturamento\"] + colunas_existentes\n",
    "\n",
    "# Seleciona as colunas na ordem desejada\n",
    "df_pag_final = df_pag.select(*colunas_finais).sort(\"id_faturamento\")\n",
    "\n",
    "# 4) Selecionar colunas para salvar\n",
    "df_final = df_pag_final.select(\"id_faturamento\",\"id_data\", \"valor_faturado\")\n",
    "\n",
    "\n",
    "# Corrigir os id_data que não terminam com '01'\n",
    "df_final = df_final.withColumn(\n",
    "    \"id_data\",\n",
    "    when(col(\"id_data\") % 100 != 1,  # verifica se os dois últimos dígitos são diferentes de 01\n",
    "         expr(\"cast(int(id_data / 100) * 100 + 1 as int)\")  # troca os dois últimos dígitos por 01\n",
    "    ).otherwise(col(\"id_data\"))  # mantém se já termina com 01\n",
    ")\n",
    "\n",
    "# 5) Salvar na camada silver (Delta)\n",
    "df_final.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_faturamento_pagseguro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4be04e-a97e-4511-a1b0-484fef0ff78a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Definindo a janela ordenada para o row_number\n",
    "window_spec = Window.orderBy(\"Data\", \"Nome__Alucar_\")\n",
    "\n",
    "# 1. Ler a tabela Delta com os dados da Alucar\n",
    "df_val = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"Tables/b_vendas_clientes_alucar\")\n",
    "    .select(\"Data\", \"Valor_Receita\", \"Nome__Alucar_\")\n",
    "    .withColumn(\"id_venda_alucar\", row_number().over(window_spec))\n",
    "    .withColumnRenamed(\"Data\", \"data\")\n",
    "    .withColumnRenamed(\"Valor_Receita\", \"valor_venda\")\n",
    "    .withColumnRenamed(\"Nome__Alucar_\", \"nome_cliente\")\n",
    ")\n",
    "\n",
    "# 2. Ler a dimensão cliente\n",
    "df_clientes = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"Tables/s_dim_cliente\")\n",
    "    .select(\"id_cliente\", \"nome_cliente\")\n",
    ")\n",
    "\n",
    "# 3. Juntar com dim_cliente\n",
    "df_val = df_val.join(df_clientes, on=\"nome_cliente\", how=\"left\")\n",
    "\n",
    "# 4. Criar id_data no formato YYYYMMDD\n",
    "df_val = df_val.withColumn(\"data_formatada\", to_date(\"data\", \"yyyy-MM-dd\"))\n",
    "df_val = df_val.withColumn(\"id_data\", date_format(\"data_formatada\", \"yyyyMMdd\").cast(IntegerType()))\n",
    "\n",
    "# 5. Converter valor_venda de R$ X.XXX,XX para float\n",
    "df_val = (\n",
    "    df_val.withColumn(\"valor_venda\", regexp_replace(\"valor_venda\", \"R\\\\$\", \"\"))\n",
    "           .withColumn(\"valor_venda\", regexp_replace(\"valor_venda\", \"\\\\.\", \"\"))\n",
    "           .withColumn(\"valor_venda\", regexp_replace(\"valor_venda\", \",\", \".\"))\n",
    "           .withColumn(\"valor_venda\", col(\"valor_venda\").cast(\"float\"))\n",
    ")\n",
    "\n",
    "# 6. Selecionar colunas finais\n",
    "df_val_final = df_val.select(\"id_venda_alucar\", \"id_cliente\", \"id_data\", \"valor_venda\")\n",
    "\n",
    "# 7. Salvar como Delta\n",
    "df_val_final.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_vendas_alucar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1f5db-db57-4893-8b33-c5808a4c4b89",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Leitura da tabela 00_vendas_clientes_consigcar da camada bronze\n",
    "df_cons = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"Tables/b_vendas_clientes_consigcar\")\n",
    "    .selectExpr(\n",
    "        \"row_number() OVER (ORDER BY Nome) as id_venda_consigcar\",  # cria id sequencial (se não existir id)\n",
    "        \"Nome as nome_cliente\",\n",
    "        \"`Tipo_Produto` as tipo_produto\",\n",
    "        \"`Quantidade_de_vezes` as num_parcelas\",\n",
    "        \"`Valor_parcela` as valor_parcela\",\n",
    "        \"Vendedor as nome_vendedor\",\n",
    "        \"`Data_do_pagamento` as data_primeira_parcela\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Se a tabela já tem uma chave única (ex: rowid), use ela para id_venda_consigcar no lugar do row_number\n",
    "\n",
    "# 2) Calcular valor_total = num_parcelas * valor_parcela\n",
    "df_cons = df_cons.withColumn(\"valor_total\", col(\"num_parcelas\") * col(\"valor_parcela\"))\n",
    "\n",
    "# 3) Converter data_primeira_parcela para date\n",
    "df_cons = df_cons.withColumn(\"data_primeira_parcela\", to_date(\"data_primeira_parcela\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# 4) Calcular data_ultima_parcela = data_primeira_parcela + (num_parcelas - 1) meses\n",
    "# Spark não tem diretamente add months com col * int, então usamos expr com INTERVAL e SQL\n",
    "df_cons = df_cons.withColumn(\n",
    "    \"data_ultima_parcela\",\n",
    "    expr(\"add_months(data_primeira_parcela, CAST(num_parcelas AS int) - 1)\")\n",
    ")\n",
    "\n",
    "# 5) Formatar as datas no formato YYYYMMDD como int\n",
    "df_cons = df_cons.withColumn(\n",
    "    \"data_primeira_parcela\", date_format(\"data_primeira_parcela\", \"yyyyMMdd\").cast(IntegerType())\n",
    ").withColumn(\n",
    "    \"data_ultima_parcela\", date_format(\"data_ultima_parcela\", \"yyyyMMdd\").cast(IntegerType())\n",
    ")\n",
    "\n",
    "# 6) Ler dim_cliente para obter id_cliente\n",
    "df_clientes = spark.read.format(\"delta\").load(\"Tables/s_dim_cliente\").select(\"id_cliente\", \"nome_cliente\")\n",
    "\n",
    "# 7) Ler dim_vendedor para obter id_vendedor\n",
    "df_vend = spark.read.format(\"delta\").load(\"Tables/s_dim_vendedor\").select(\"id_vendedor\", \"nome_vendedor\")\n",
    "\n",
    "# 8) Fazer join para incluir id_cliente e id_vendedor\n",
    "df_cons = df_cons.join(df_clientes, on=\"nome_cliente\", how=\"left\")\n",
    "df_cons = df_cons.join(df_vend, on=\"nome_vendedor\", how=\"left\")\n",
    "\n",
    "# 9) Selecionar colunas relevantes\n",
    "colunas_fato = [\n",
    "    \"id_venda_consigcar\", \"id_cliente\", \"tipo_produto\", \"id_vendedor\",\n",
    "    \"num_parcelas\", \"valor_parcela\", \"valor_total\",\n",
    "    \"data_primeira_parcela\", \"data_ultima_parcela\"\n",
    "]\n",
    "df_fato = df_cons.select(colunas_fato)\n",
    "\n",
    "# 10) Salvar tabela fato na camada silver\n",
    "df_fato.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_vendas_consigcar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56cb789-b0f0-429b-9bdb-6053f61387a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Ler fato_vendas_consigcar da camada silver\n",
    "df_vendas = spark.read.format(\"delta\").load(\"Tables/s_fato_vendas_consigcar\")\n",
    "\n",
    "# 2) Ler dim_tempo da camada silver\n",
    "df_tempo = spark.read.format(\"delta\").load(\"Tables/s_dim_tempo\")\n",
    "\n",
    "# 3) Fazer join entre fato_vendas_consigcar e dim_tempo\n",
    "# Aqui usamos cast para comparar os IDs corretamente\n",
    "df_joined = df_vendas.join(\n",
    "    df_tempo,\n",
    "    df_vendas[\"data_primeira_parcela\"].cast(\"string\") == df_tempo[\"id_data\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# 4) Agregar: total de vendas e soma de valor_total por id_vendedor e id_data\n",
    "df_vendas_diaria = (\n",
    "    df_joined.groupBy(\"id_vendedor\", \"id_data\",\"valor_parcela\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_vendas\"),\n",
    "        spark_sum(\"valor_total\").alias(\"valor_total\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) Salvar a tabela na camada silver\n",
    "df_vendas_diaria.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_vendas_diaria_vendedor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01662c86-ede3-445a-b0bb-e23f2c29e4da",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Carregar os dados da fato_vendas_consigcar\n",
    "df_vendas = spark.read.format(\"delta\").load(\"Tables/s_fato_vendas_consigcar\")\n",
    "\n",
    "# 2. Converter data_primeira_parcela (formato Int YYYYMMDD) para data real\n",
    "df_vendas = df_vendas.withColumn(\n",
    "    \"data_inicial\",\n",
    "    to_date(col(\"data_primeira_parcela\").cast(\"string\"), \"yyyyMMdd\")\n",
    ")\n",
    "\n",
    "# 3. Criar a sequência de datas mensais para cada parcela\n",
    "df_vendas = df_vendas.withColumn(\n",
    "    \"datas_pagamento\",\n",
    "    sequence(\n",
    "        col(\"data_inicial\"),\n",
    "        expr(\"add_months(data_inicial, num_parcelas - 1)\"),\n",
    "        expr(\"interval 1 month\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Explodir as datas e formatar como id_data (YYYYMMDD)\n",
    "df_pagamentos = df_vendas.select(\n",
    "    col(\"id_venda_consigcar\"),\n",
    "    col(\"valor_parcela\").alias(\"valor\"),\n",
    "    explode(\"datas_pagamento\").alias(\"data_pagamento\")\n",
    ").withColumn(\n",
    "    \"id_data\", date_format(col(\"data_pagamento\"), \"yyyyMMdd\").cast(IntegerType())\n",
    ").select(\"id_venda_consigcar\", \"valor\", \"id_data\")\n",
    "\n",
    "# 5. Salvar como tabela Delta na camada silver\n",
    "df_pagamentos.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_dim_pagamentos_programados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af5ac5-04d8-4ccc-a5f3-069b22804ef6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializa a SparkSession (se ainda não o fez)\n",
    "spark = SparkSession.builder.appName(\"MetasAlucar\").getOrCreate()\n",
    "\n",
    "# 1. Carregar os dados da tabela \"00_metas_plr\"\n",
    "df_metas = spark.read.format(\"delta\").load(\"Tables/b_metas_plr\")\n",
    "\n",
    "# 2. Converter a coluna Data para id_data (formato Int: YYYYMMDD)\n",
    "df_metas = df_metas.withColumn(\"id_data\", date_format(to_date(col(\"Data\")), \"yyyyMMdd\").cast(\"int\"))\n",
    "\n",
    "# 3. Selecionar e renomear colunas\n",
    "df_metas = df_metas.select(\n",
    "    \"id_data\",\n",
    "    col(\"Meta_1_ALUCAR\").alias(\"meta_vendas_1_cum\"),\n",
    "    col(\"Meta_2_ALUCAR\").alias(\"meta_vendas_2_cum\")\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Calcular metas mensais com .diff() via função lag\n",
    "#    Definindo default=0 para 'lag()' para que a primeira linha de 'meta_vendas_X_mes'\n",
    "#    seja igual ao 'meta_vendas_X_cum' correspondente.\n",
    "window_spec = Window.orderBy(\"id_data\")\n",
    "\n",
    "df_metas = df_metas.withColumn(\n",
    "    \"meta_vendas_1_mes\",\n",
    "    col(\"meta_vendas_1_cum\") - lag(\"meta_vendas_1_cum\", 1, 0).over(window_spec) # <-- Default agora é 0\n",
    ").withColumn(\n",
    "    \"meta_vendas_2_mes\",\n",
    "    col(\"meta_vendas_2_cum\") - lag(\"meta_vendas_2_cum\", 1, 0).over(window_spec) # <-- Default agora é 0\n",
    ")\n",
    "\n",
    "# ---\n",
    "\n",
    "# 5. Para o primeiro mês do ano (janeiro), usar o valor cumulativo diretamente\n",
    "#    Esta lógica ainda é útil se suas metas cumulativas resetam anualmente.\n",
    "df_metas = df_metas.withColumn(\n",
    "    \"mes\",\n",
    "    date_format(col(\"id_data\").cast(\"string\"), \"MMdd\").substr(1, 2)\n",
    ")\n",
    "\n",
    "df_metas = df_metas.withColumn(\n",
    "    \"meta_vendas_1_mes\",\n",
    "    when(col(\"mes\") == \"01\", col(\"meta_vendas_1_cum\")).otherwise(col(\"meta_vendas_1_mes\"))\n",
    ").withColumn(\n",
    "    \"meta_vendas_2_mes\",\n",
    "    when(col(\"mes\") == \"01\", col(\"meta_vendas_2_cum\")).otherwise(col(\"meta_vendas_2_mes\"))\n",
    ").drop(\"mes\")\n",
    "\n",
    "# ---\n",
    "\n",
    "# 6. Salvar a tabela como Delta na camada Silver\n",
    "df_metas.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_metas_alucar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1dd5e-80f9-41b2-9edb-ccc144281ace",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession if not already done\n",
    "spark = SparkSession.builder.appName(\"MetasConsigcar\").getOrCreate()\n",
    "\n",
    "# 1. Load data from the \"00_metas_plr\" table\n",
    "df_metas_consig = spark.read.format(\"delta\").load(\"Tables/b_metas_plr\")\n",
    "\n",
    "# 2. Convert the 'Data' column to 'id_data' (YYYYMMDD int)\n",
    "df_metas_consig = df_metas_consig.withColumn(\n",
    "    \"id_data\",\n",
    "    date_format(col(\"Data\"), \"yyyyMMdd\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# 3. Select and rename columns\n",
    "df_metas_consig = df_metas_consig.select(\n",
    "    \"id_data\",\n",
    "    col(\"Meta_1_ConsigCar\").alias(\"meta_vendas_1_cum\"),\n",
    "    col(\"Meta_2_ConsigCar\").alias(\"meta_vendas_2_cum\")\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Calculate monthly targets using lag (difference)\n",
    "#    Setting default=0 for lag ensures that the first row's 'mes' value\n",
    "#    is equal to its 'cum' value (cum - 0 = cum).\n",
    "window_spec = Window.orderBy(\"id_data\")\n",
    "\n",
    "df_metas_consig = df_metas_consig.withColumn(\n",
    "    \"meta_vendas_1_mes\",\n",
    "    col(\"meta_vendas_1_cum\") - lag(\"meta_vendas_1_cum\", 1, 0).over(window_spec) # <-- Default agora é 0\n",
    ").withColumn(\n",
    "    \"meta_vendas_2_mes\",\n",
    "    col(\"meta_vendas_2_cum\") - lag(\"meta_vendas_2_cum\", 1, 0).over(window_spec) # <-- Default agora é 0\n",
    ")\n",
    "\n",
    "# ---\n",
    "\n",
    "# 5. Adjust for the first month of EACH YEAR (January)\n",
    "#    This logic will override the above calculation if it's January and\n",
    "#    you want the monthly value to be the cumulative for that start of year.\n",
    "df_metas_consig = df_metas_consig.withColumn(\n",
    "    \"mes\",\n",
    "    date_format(col(\"id_data\").cast(\"string\"), \"MMdd\").substr(1, 2)\n",
    ")\n",
    "\n",
    "df_metas_consig = df_metas_consig.withColumn(\n",
    "    \"meta_vendas_1_mes\",\n",
    "    when(col(\"mes\") == \"01\", col(\"meta_vendas_1_cum\")).otherwise(col(\"meta_vendas_1_mes\"))\n",
    ").withColumn(\n",
    "    \"meta_vendas_2_mes\",\n",
    "    when(col(\"mes\") == \"01\", col(\"meta_vendas_2_cum\")).otherwise(col(\"meta_vendas_2_mes\"))\n",
    ").drop(\"mes\")\n",
    "\n",
    "# ---\n",
    "\n",
    "# 6. Save the resulting table to the Silver layer (Delta format)\n",
    "df_metas_consig.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_metas_consigcar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b61bd-95be-4121-b9bf-105fd475ecf7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Carregar os dados da tabela \"00_vendas_clientes_alucar_estimativa\"\n",
    "df_vendas_estimativa = spark.read.format(\"delta\").load(\"Tables/b_vendas_clientes_alucar_estimativa\")\n",
    "\n",
    "# 2. Converter a coluna Data para id_data (YYYYMMDD int)\n",
    "df_vendas_estimativa = df_vendas_estimativa.withColumn(\n",
    "    \"id_data\",\n",
    "    date_format(col(\"Data\"), \"yyyyMMdd\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# 3. Selecionar e renomear colunas\n",
    "df_vendas_estimativa = df_vendas_estimativa.select(\n",
    "    \"id_data\",\n",
    "    col(\"Nome__Alucar_\").alias(\"nome\"),\n",
    "    col(\"Valor_Receita\").alias(\"valor_receita_estimativa\")\n",
    ")\n",
    "\n",
    "# 4. Remover 'R$', pontos e substituir vírgula por ponto para conversão numérica\n",
    "df_vendas_estimativa = df_vendas_estimativa.withColumn(\n",
    "    \"valor_receita_estimativa\",\n",
    "    regexp_replace(col(\"valor_receita_estimativa\"), \"R\\\\$\", \"\")  # Remove 'R$'\n",
    ")\n",
    "\n",
    "df_vendas_estimativa = df_vendas_estimativa.withColumn(\n",
    "    \"valor_receita_estimativa\",\n",
    "    regexp_replace(col(\"valor_receita_estimativa\"), \"\\\\.\", \"\")  # Remove pontos de milhar\n",
    ")\n",
    "\n",
    "df_vendas_estimativa = df_vendas_estimativa.withColumn(\n",
    "    \"valor_receita_estimativa\",\n",
    "    regexp_replace(col(\"valor_receita_estimativa\"), \",\", \".\")  # Troca vírgula por ponto decimal\n",
    ")\n",
    "\n",
    "# 5. Converter para float\n",
    "df_vendas_estimativa = df_vendas_estimativa.withColumn(\n",
    "    \"valor_receita_estimativa\",\n",
    "    col(\"valor_receita_estimativa\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# 6. Salvar a tabela no formato Delta ou conforme sua arquitetura\n",
    "df_vendas_estimativa.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_vendas_clientes_alucar_estimativa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152e2be-1926-40dc-88a5-c7d9c1a16708",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Carregar os dados da tabela \"00_receita_consigcar_estimativa\"\n",
    "df_consig_estimativa = spark.read.format(\"delta\").load(\"Tables/b_receita_consigcar_estimativa\")\n",
    "\n",
    "# 2. Converter a coluna Data para id_data no formato YYYYMMDD (inteiro)\n",
    "df_consig_estimativa = df_consig_estimativa.withColumn(\n",
    "    \"id_data\",\n",
    "    date_format(col(\"Data\"), \"yyyyMMdd\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# 3. Selecionar e renomear colunas\n",
    "df_consig_estimativa = df_consig_estimativa.select(\n",
    "    \"id_data\",\n",
    "    col(\"Valor\").alias(\"valor_receita_estimativa\")\n",
    ")\n",
    "\n",
    "# 4. Remover 'R$', pontos e substituir vírgula por ponto para conversão numérica\n",
    "df_consig_estimativa = df_consig_estimativa.withColumn(\n",
    "    \"valor_receita_estimativa\",\n",
    "    regexp_replace(col(\"valor_receita_estimativa\"), \"R\\\\$\", \"\")\n",
    ")\n",
    "\n",
    "df_consig_estimativa = df_consig_estimativa.withColumn(\n",
    "    \"valor_receita_estimativa\",\n",
    "    regexp_replace(col(\"valor_receita_estimativa\"), \"\\\\.\", \"\")\n",
    ")\n",
    "\n",
    "df_consig_estimativa = df_consig_estimativa.withColumn(\n",
    "    \"valor_receita_estimativa\",\n",
    "    regexp_replace(col(\"valor_receita_estimativa\"), \",\", \".\")\n",
    ")\n",
    "\n",
    "# 5. Converter para float\n",
    "df_consig_estimativa = df_consig_estimativa.withColumn(\n",
    "    \"valor_receita_estimativa\",\n",
    "    col(\"valor_receita_estimativa\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# 6. Salvar a tabela no formato Delta ou conforme sua arquitetura\n",
    "df_consig_estimativa.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/s_fato_consigcar_estimativa\")\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "4a780a82-5b79-440f-b832-289c5aa59c04",
    "default_lakehouse_name": "lakehouse_vendas",
    "default_lakehouse_workspace_id": "9192c1ef-ef1d-4e27-8495-7193eb001d77",
    "known_lakehouses": [
     {
      "id": "4a780a82-5b79-440f-b832-289c5aa59c04"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
